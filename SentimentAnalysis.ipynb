{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f72586eb-2dbd-4c4c-9398-8981d550e719",
      "metadata": {
        "id": "f72586eb-2dbd-4c4c-9398-8981d550e719",
        "outputId": "e4f5f381-8a67-4bcb-9995-c2cd27ca6485",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'sentimentAnalysis' already exists and is not an empty directory.\n",
            "/content/sentimentAnalysis\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Mateusz-Wojciechowski/sentimentAnalysis.git\n",
        "%cd sentimentAnalysis"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install 'portalocker>=2.0.0'"
      ],
      "metadata": {
        "id": "cwTXPiescQs2",
        "outputId": "2047cccc-6826-48e8-9629-2fa4699c512f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "cwTXPiescQs2",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: portalocker>=2.0.0 in /usr/local/lib/python3.10/dist-packages (2.8.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchtext.datasets import IMDB\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from functools import partial\n",
        "from SentimentModel import SentimentModel\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "H5fF_83CbW1l"
      },
      "id": "H5fF_83CbW1l",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21e3a902-4cd3-43b0-a1eb-dbb8ac5dd135",
      "metadata": {
        "id": "21e3a902-4cd3-43b0-a1eb-dbb8ac5dd135",
        "outputId": "28112cf7-f30f-4366-da19-bba806f16b87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "batch 0\n",
            "batch 10\n",
            "batch 20\n",
            "batch 30\n",
            "batch 40\n",
            "batch 50\n",
            "batch 60\n",
            "batch 70\n",
            "batch 80\n",
            "batch 90\n",
            "batch 100\n",
            "batch 110\n",
            "batch 120\n",
            "batch 130\n",
            "batch 140\n",
            "batch 150\n",
            "batch 160\n",
            "batch 170\n",
            "batch 180\n",
            "batch 190\n",
            "batch 200\n",
            "batch 210\n",
            "batch 220\n",
            "batch 230\n",
            "batch 240\n",
            "batch 250\n",
            "batch 260\n",
            "batch 270\n",
            "batch 280\n",
            "batch 290\n",
            "batch 300\n",
            "batch 310\n",
            "batch 320\n",
            "batch 330\n",
            "batch 340\n",
            "Loss in epoch 0 is 0.4778189799710191\n",
            "batch 0\n",
            "batch 10\n",
            "batch 20\n",
            "batch 30\n",
            "batch 40\n",
            "batch 50\n",
            "batch 60\n",
            "batch 70\n",
            "batch 80\n",
            "batch 90\n",
            "batch 100\n",
            "batch 110\n",
            "batch 120\n",
            "batch 130\n",
            "batch 140\n",
            "batch 150\n",
            "batch 160\n",
            "batch 170\n",
            "batch 180\n",
            "batch 190\n",
            "batch 200\n",
            "batch 210\n",
            "batch 220\n",
            "batch 230\n",
            "batch 240\n",
            "batch 250\n",
            "batch 260\n",
            "batch 270\n",
            "batch 280\n",
            "batch 290\n",
            "batch 300\n",
            "batch 310\n",
            "batch 320\n",
            "batch 330\n",
            "batch 340\n",
            "Loss in epoch 1 is 4.462897280177458e-05\n",
            "batch 0\n",
            "batch 10\n",
            "batch 20\n",
            "batch 30\n",
            "batch 40\n",
            "batch 50\n",
            "batch 60\n",
            "batch 70\n",
            "batch 80\n",
            "batch 90\n",
            "batch 100\n",
            "batch 110\n",
            "batch 120\n",
            "batch 130\n",
            "batch 140\n",
            "batch 150\n",
            "batch 160\n",
            "batch 170\n",
            "batch 180\n",
            "batch 190\n",
            "batch 200\n",
            "batch 210\n",
            "batch 220\n",
            "batch 230\n",
            "batch 240\n",
            "batch 250\n",
            "batch 260\n",
            "batch 270\n",
            "batch 280\n",
            "batch 290\n",
            "batch 300\n",
            "batch 310\n",
            "batch 320\n",
            "batch 330\n",
            "batch 340\n",
            "Loss in epoch 2 is 4.3783331904023726e-05\n",
            "batch 0\n",
            "batch 10\n",
            "batch 20\n",
            "batch 30\n",
            "batch 40\n",
            "batch 50\n",
            "batch 60\n",
            "batch 70\n",
            "batch 80\n",
            "batch 90\n",
            "batch 100\n",
            "batch 110\n",
            "batch 120\n",
            "batch 130\n",
            "batch 140\n",
            "batch 150\n",
            "batch 160\n",
            "batch 170\n",
            "batch 180\n",
            "batch 190\n",
            "batch 200\n",
            "batch 210\n",
            "batch 220\n",
            "batch 230\n",
            "batch 240\n",
            "batch 250\n",
            "batch 260\n",
            "batch 270\n",
            "batch 280\n",
            "batch 290\n",
            "batch 300\n",
            "batch 310\n",
            "batch 320\n",
            "batch 330\n",
            "batch 340\n",
            "Loss in epoch 3 is 4.3176109606690716e-05\n",
            "batch 0\n",
            "batch 10\n",
            "batch 20\n",
            "batch 30\n",
            "batch 40\n",
            "batch 50\n",
            "batch 60\n",
            "batch 70\n",
            "batch 80\n",
            "batch 90\n",
            "batch 100\n",
            "batch 110\n",
            "batch 120\n",
            "batch 130\n",
            "batch 140\n",
            "batch 150\n",
            "batch 160\n",
            "batch 170\n",
            "batch 180\n",
            "batch 190\n",
            "batch 200\n",
            "batch 210\n",
            "batch 220\n",
            "batch 230\n",
            "batch 240\n",
            "batch 250\n",
            "batch 260\n",
            "batch 270\n",
            "batch 280\n",
            "batch 290\n",
            "batch 300\n",
            "batch 310\n",
            "batch 320\n",
            "batch 330\n",
            "batch 340\n",
            "Loss in epoch 4 is 4.218518399312643e-05\n",
            "batch 0\n",
            "batch 10\n",
            "batch 20\n",
            "batch 30\n",
            "batch 40\n",
            "batch 50\n",
            "batch 60\n",
            "batch 70\n",
            "batch 80\n",
            "batch 90\n",
            "batch 100\n",
            "batch 110\n",
            "batch 120\n",
            "batch 130\n",
            "batch 140\n",
            "batch 150\n",
            "batch 160\n",
            "batch 170\n",
            "batch 180\n",
            "batch 190\n",
            "batch 200\n",
            "batch 210\n",
            "batch 220\n",
            "batch 230\n",
            "batch 240\n",
            "batch 250\n",
            "batch 260\n",
            "batch 270\n",
            "batch 280\n",
            "batch 290\n",
            "batch 300\n",
            "batch 310\n",
            "batch 320\n",
            "batch 330\n",
            "batch 340\n",
            "Loss in epoch 5 is 4.17456005621375e-05\n",
            "batch 0\n",
            "batch 10\n",
            "batch 20\n",
            "batch 30\n",
            "batch 40\n",
            "batch 50\n",
            "batch 60\n",
            "batch 70\n",
            "batch 80\n",
            "batch 90\n",
            "batch 100\n",
            "batch 110\n",
            "batch 120\n",
            "batch 130\n",
            "batch 140\n",
            "batch 150\n",
            "batch 160\n",
            "batch 170\n",
            "batch 180\n",
            "batch 190\n",
            "batch 200\n",
            "batch 210\n",
            "batch 220\n",
            "batch 230\n",
            "batch 240\n",
            "batch 250\n",
            "batch 260\n",
            "batch 270\n",
            "batch 280\n",
            "batch 290\n",
            "batch 300\n",
            "batch 310\n",
            "batch 320\n",
            "batch 330\n",
            "batch 340\n",
            "Loss in epoch 6 is 4.172324885587386e-05\n",
            "batch 0\n",
            "batch 10\n",
            "batch 20\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-3ef885ce3be5>\u001b[0m in \u001b[0;36m<cell line: 44>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m       \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 492\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "# Tokenizer\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "\n",
        "def yield_tokens(data_iter):\n",
        "    for text in data_iter:\n",
        "        yield tokenizer(text)\n",
        "\n",
        "def process_text(text, vocab):\n",
        "    return torch.tensor(vocab(tokenizer(text)), dtype=torch.long)\n",
        "\n",
        "def collate_batch(batch):\n",
        "    label_list, text_list = [], []\n",
        "    for label, text in batch:\n",
        "        label_tensor = torch.tensor([label-1], dtype=torch.float)\n",
        "        processed_text = process_text(text, vocab)\n",
        "        label_list.append(label_tensor)\n",
        "        text_list.append(processed_text)\n",
        "    return torch.stack(label_list), pad_sequence(text_list, padding_value=vocab[\"<pad>\"], batch_first=True)\n",
        "\n",
        "def calculate_accuracy(preds, y):\n",
        "    preds = torch.sigmoid(preds)\n",
        "    rounded_preds = torch.round(preds)\n",
        "    correct = (rounded_preds == y).float()\n",
        "    accuracy = correct.sum() / len(correct)\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "train_data = list(IMDB(split='train'))\n",
        "random.shuffle(train_data)\n",
        "\n",
        "vocab = build_vocab_from_iterator(yield_tokens(data_iter for label, data_iter in train_data), specials=[\"<unk>\", \"<pad>\"])\n",
        "vocab.set_default_index(vocab[\"<unk>\"])\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=32, shuffle=False, collate_fn=collate_batch)\n",
        "\n",
        "test_data = list(IMDB(split='test'))\n",
        "random.shuffle(test_data)\n",
        "test_loader = DataLoader(test_data, batch_size=32, shuffle=False, collate_fn=collate_batch)\n",
        "\n",
        "d_model = 512\n",
        "num_heads = 8\n",
        "max_seq_len = 5000\n",
        "d_ff = 2048\n",
        "learning_rate = 0.001\n",
        "num_classes = 2\n",
        "vocab_size = len(vocab)\n",
        "num_epochs = 100\n",
        "\n",
        "\n",
        "model = SentimentModel(d_model, d_ff, num_heads, max_seq_len, num_classes, vocab_size)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch: {epoch + 1}\")\n",
        "    total_loss = 0\n",
        "    total_accuracy = 0\n",
        "    total_examples = 0\n",
        "\n",
        "    for labels, sequences in train_loader:\n",
        "        labels, sequences = labels.to(device), sequences.to(device)\n",
        "\n",
        "        output = model(sequences)\n",
        "        loss = loss_fn(output, labels)\n",
        "        accuracy = calculate_accuracy(output, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_accuracy += accuracy.item()\n",
        "        total_examples += labels.size(0)\n",
        "\n",
        "    print(f\"Loss in epoch {epoch + 1} is {total_loss}\")\n",
        "    print(f\"Accuracy in epoch {epoch + 1} is {total_accuracy / total_examples}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}