{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f72586eb-2dbd-4c4c-9398-8981d550e719",
      "metadata": {
        "id": "f72586eb-2dbd-4c4c-9398-8981d550e719",
        "outputId": "69f47efa-34cc-4162-952e-350bc956614d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'sentimentAnalysis' already exists and is not an empty directory.\n",
            "/content/sentimentAnalysis\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Mateusz-Wojciechowski/sentimentAnalysis.git\n",
        "%cd sentimentAnalysis"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install 'portalocker>=2.0.0'"
      ],
      "metadata": {
        "id": "cwTXPiescQs2",
        "outputId": "f836a8e0-15e7-4daa-da64-3683ce238a45",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "cwTXPiescQs2",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: portalocker>=2.0.0 in /usr/local/lib/python3.10/dist-packages (2.8.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchtext.datasets import IMDB\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from SentimentModel import SentimentModel\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "\n",
        "def yield_tokens(data_iter):\n",
        "    for text in data_iter:\n",
        "        yield tokenizer(text)\n",
        "\n",
        "def process_text(text, vocab):\n",
        "    return torch.tensor(vocab(tokenizer(text)), dtype=torch.long).to(device)\n",
        "\n",
        "def collate_batch(batch):\n",
        "    label_list, text_list = [], []\n",
        "    for label, text in batch:\n",
        "        label_tensor = torch.tensor([label-1], dtype=torch.float).to(device)\n",
        "        processed_text = process_text(text, vocab)\n",
        "        label_list.append(label_tensor)\n",
        "        text_list.append(processed_text)\n",
        "    return torch.stack(label_list).to(device), pad_sequence(text_list, padding_value=vocab[\"<pad>\"], batch_first=True).to(device)\n",
        "\n",
        "def calculate_accuracy(preds, y):\n",
        "    preds = torch.sigmoid(preds)\n",
        "    rounded_preds = torch.round(preds)\n",
        "    correct = (rounded_preds == y).float()\n",
        "    accuracy = correct.sum() / len(correct)\n",
        "    return accuracy\n",
        "\n",
        "train_data = list(IMDB(split='train'))\n",
        "random.shuffle(train_data)\n",
        "\n",
        "vocab = build_vocab_from_iterator(yield_tokens(data_iter for label, data_iter in train_data), specials=[\"<unk>\", \"<pad>\"])\n",
        "vocab.set_default_index(vocab[\"<unk>\"])\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=8, shuffle=False, collate_fn=collate_batch)\n",
        "\n",
        "test_data = list(IMDB(split='test'))\n",
        "random.shuffle(test_data)\n",
        "test_loader = DataLoader(test_data, batch_size=32, shuffle=False, collate_fn=collate_batch)\n",
        "\n",
        "d_model = 512\n",
        "num_heads = 8\n",
        "max_seq_len = 5000\n",
        "d_ff = 2048\n",
        "learning_rate = 0.001\n",
        "num_classes = 1\n",
        "vocab_size = len(vocab)\n",
        "num_epochs = 100\n",
        "\n",
        "model = SentimentModel(d_model, d_ff, num_heads, max_seq_len, num_classes, vocab_size)\n",
        "model.to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch: {epoch + 1}\")\n",
        "    total_loss = 0\n",
        "    total_accuracy = 0\n",
        "    total_examples = 0\n",
        "    i = 0\n",
        "\n",
        "    model.train()\n",
        "    for labels, sequences in train_loader:\n",
        "        if i% 100 == 0:\n",
        "          print(f\"batch {i}\")\n",
        "\n",
        "        i +=1\n",
        "        output = model(sequences)\n",
        "        loss = loss_fn(output, labels)\n",
        "        accuracy = calculate_accuracy(output, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_accuracy += accuracy.item()\n",
        "        total_examples += labels.size(0)\n",
        "\n",
        "    print(f\"Loss in epoch {epoch + 1} is {total_loss}\")\n",
        "    print(f\"Accuracy in epoch {epoch + 1} is {total_accuracy / total_examples}\")\n"
      ],
      "metadata": {
        "id": "srjqM4K-Mheh",
        "outputId": "f85498e8-99bc-4fbd-aada-c8a1f2578c94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "id": "srjqM4K-Mheh",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Epoch: 1\n",
            "batch 0\n",
            "batch 100\n",
            "batch 200\n",
            "batch 300\n",
            "batch 400\n",
            "batch 500\n",
            "batch 600\n",
            "batch 700\n",
            "batch 800\n",
            "batch 900\n",
            "batch 1000\n",
            "batch 1100\n",
            "batch 1200\n",
            "batch 1300\n",
            "batch 1400\n",
            "batch 1500\n",
            "batch 1600\n",
            "batch 1700\n",
            "batch 1800\n",
            "batch 1900\n",
            "batch 2000\n",
            "batch 2100\n",
            "batch 2200\n",
            "batch 2300\n",
            "batch 2400\n",
            "batch 2500\n",
            "batch 2600\n",
            "batch 2700\n",
            "batch 2800\n",
            "batch 2900\n",
            "batch 3000\n",
            "batch 3100\n",
            "Loss in epoch 1 is 1867.0120996832848\n",
            "Accuracy in epoch 1 is 0.08358\n",
            "Epoch: 2\n",
            "batch 0\n",
            "batch 100\n",
            "batch 200\n",
            "batch 300\n",
            "batch 400\n",
            "batch 500\n",
            "batch 600\n",
            "batch 700\n",
            "batch 800\n",
            "batch 900\n",
            "batch 1000\n",
            "batch 1100\n",
            "batch 1200\n",
            "batch 1300\n",
            "batch 1400\n",
            "batch 1500\n",
            "batch 1600\n",
            "batch 1700\n",
            "batch 1800\n",
            "batch 1900\n",
            "batch 2000\n",
            "batch 2100\n",
            "batch 2200\n",
            "batch 2300\n",
            "batch 2400\n",
            "batch 2500\n",
            "batch 2600\n",
            "batch 2700\n",
            "batch 2800\n",
            "batch 2900\n",
            "batch 3000\n",
            "batch 3100\n",
            "Loss in epoch 2 is 1201.5547805679962\n",
            "Accuracy in epoch 2 is 0.104435\n",
            "Epoch: 3\n",
            "batch 0\n",
            "batch 100\n",
            "batch 200\n",
            "batch 300\n",
            "batch 400\n",
            "batch 500\n",
            "batch 600\n",
            "batch 700\n",
            "batch 800\n",
            "batch 900\n",
            "batch 1000\n",
            "batch 1100\n",
            "batch 1200\n",
            "batch 1300\n",
            "batch 1400\n",
            "batch 1500\n",
            "batch 1600\n",
            "batch 1700\n",
            "batch 1800\n",
            "batch 1900\n",
            "batch 2000\n",
            "batch 2100\n",
            "batch 2200\n",
            "batch 2300\n",
            "batch 2400\n",
            "batch 2500\n",
            "batch 2600\n",
            "batch 2700\n",
            "batch 2800\n",
            "batch 2900\n",
            "batch 3000\n",
            "batch 3100\n",
            "Loss in epoch 3 is 929.8334339023568\n",
            "Accuracy in epoch 3 is 0.11029\n",
            "Epoch: 4\n",
            "batch 0\n",
            "batch 100\n",
            "batch 200\n",
            "batch 300\n",
            "batch 400\n",
            "batch 500\n",
            "batch 600\n",
            "batch 700\n",
            "batch 800\n",
            "batch 900\n",
            "batch 1000\n",
            "batch 1100\n",
            "batch 1200\n",
            "batch 1300\n",
            "batch 1400\n",
            "batch 1500\n",
            "batch 1600\n",
            "batch 1700\n",
            "batch 1800\n",
            "batch 1900\n",
            "batch 2000\n",
            "batch 2100\n",
            "batch 2200\n",
            "batch 2300\n",
            "batch 2400\n",
            "batch 2500\n",
            "batch 2600\n",
            "batch 2700\n",
            "batch 2800\n",
            "batch 2900\n",
            "batch 3000\n",
            "batch 3100\n",
            "Loss in epoch 4 is 723.5500822014874\n",
            "Accuracy in epoch 4 is 0.114025\n",
            "Epoch: 5\n",
            "batch 0\n",
            "batch 100\n",
            "batch 200\n",
            "batch 300\n",
            "batch 400\n",
            "batch 500\n",
            "batch 600\n",
            "batch 700\n",
            "batch 800\n",
            "batch 900\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-dc09c1a34b92>\u001b[0m in \u001b[0;36m<cell line: 68>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0mtotal_accuracy\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mtotal_examples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "PfZKBpyiMfsz"
      },
      "id": "PfZKBpyiMfsz"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}